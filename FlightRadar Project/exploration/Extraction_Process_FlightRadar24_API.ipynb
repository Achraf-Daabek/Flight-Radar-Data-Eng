{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26a19d6-67b3-424a-a3c2-894504fa53d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting FlightRadarAPI\n  Using cached flightradarapi-1.3.10-py3-none-any.whl (13 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from FlightRadarAPI) (2.27.1)\nCollecting brotli\n  Using cached Brotli-1.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->FlightRadarAPI) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->FlightRadarAPI) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->FlightRadarAPI) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->FlightRadarAPI) (2021.10.8)\nInstalling collected packages: brotli, FlightRadarAPI\nSuccessfully installed FlightRadarAPI-1.3.10 brotli-1.1.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install FlightRadarAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb913e21-de74-4db6-96b9-d32fc02de991",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting country_converter\n  Using cached country_converter-1.0.0-py3-none-any.whl (44 kB)\nRequirement already satisfied: pandas>=1.0 in /databricks/python3/lib/python3.9/site-packages (from country_converter) (1.4.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0->country_converter) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0->country_converter) (2021.3)\nRequirement already satisfied: numpy>=1.18.5 in /databricks/python3/lib/python3.9/site-packages (from pandas>=1.0->country_converter) (1.21.5)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.0->country_converter) (1.16.0)\nInstalling collected packages: country-converter\nSuccessfully installed country-converter-1.0.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install country_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bbb52dc-fb63-4305-859c-86ed4eb7f94c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting geopy\n  Using cached geopy-2.4.0-py3-none-any.whl (125 kB)\nCollecting geographiclib<3,>=1.52\n  Using cached geographiclib-2.0-py3-none-any.whl (40 kB)\nInstalling collected packages: geographiclib, geopy\nSuccessfully installed geographiclib-2.0 geopy-2.4.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8979758-116f-4fd0-8140-81564f93819d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from FlightRadar24.api import FlightRadar24API\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType , FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "import logging\n",
    "import os\n",
    "from geopy.distance import geodesic\n",
    "from geopy.distance import great_circle\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "import country_converter as coco\n",
    "from pyspark.sql.functions import col, count, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df502ff4-4ddc-4490-9f97-12c393492cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configuration de base pour le logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f9a8a9-6b9b-4e12-a299-7d56de92b94d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Instance API\n",
    "fr_api = FlightRadar24API() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce3e489-e764-4b13-907d-a49ab89afc9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Premiere Etape - Passage des données de Bronze à Silver ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e8fc1d-76bc-4998-bd72-f2071b69834a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Extraction - Verification des schema - Nettoyage des données \n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, fr_api: FlightRadar24API):\n",
    "        self.fr_api = fr_api\n",
    "        \n",
    "    def extract_data(self):\n",
    "        \"\"\"A function to extract data from the API\"\"\" \n",
    "        flights = self.fr_api.get_flights() \n",
    "        airlines = self.fr_api.get_airlines()\n",
    "        zones = self.fr_api.get_zones()\n",
    "        return {\n",
    "            \"flights\": flights,\n",
    "            \"airlines\": airlines,\n",
    "            \"zones\": zones\n",
    "        }   \n",
    "    def extract_data_from_dic(self):\n",
    "        try:\n",
    "            logging.info(\"Starting data extraction from API...\")\n",
    "            airports = self.fr_api.get_airports()\n",
    "            zones = self.fr_api.get_zones()\n",
    "            logging.info(\"Data extraction completed successfully.\")\n",
    "\n",
    "            return {\n",
    "                \"airports\": airports,\n",
    "                \"zones\": zones\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Data extraction failed: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def convert_airports_to_df(self, api_data):\n",
    "        airport_data = []\n",
    "\n",
    "        for airport in api_data:\n",
    "            country = str(airport.country)\n",
    "            code_iata = str(airport.iata)\n",
    "            code_icao = str(airport.icao)\n",
    "            name = str(airport.name)\n",
    "            altitude = int(airport.altitude)\n",
    "            latitude = float(airport.latitude)\n",
    "            longitude = float(airport.longitude)\n",
    "\n",
    "            airport_data.append(Row(country, code_iata, code_icao, name, altitude, latitude, longitude))\n",
    "\n",
    "        df = spark.createDataFrame(airport_data, [\"Country\", \"IATA Code\", \"ICAO Code\", \"Name\", \"Altitude\", \"Latitude\", \"Longitude\"])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def convert_zones_to_df(self, zones):\n",
    "        zone_data = []\n",
    "\n",
    "        def process_zone(name, zone_info):\n",
    "            tl_y = float(zone_info['tl_y'])\n",
    "            tl_x = float(zone_info['tl_x'])\n",
    "            br_y = float(zone_info['br_y'])\n",
    "            br_x = float(zone_info['br_x'])\n",
    "            zone_data.append(Row(name, tl_y, tl_x, br_y, br_x))\n",
    "\n",
    "            if 'subzones' in zone_info:\n",
    "                for subname, subzone_info in zone_info['subzones'].items():\n",
    "                    process_zone(subname, subzone_info)\n",
    "\n",
    "        for zone_name, zone_info in zones.items():\n",
    "            process_zone(zone_name, zone_info)\n",
    "\n",
    "        df = spark.createDataFrame(zone_data, [\"Zone Name\", \"TL_Y\", \"TL_X\", \"BR_Y\", \"BR_X\"])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def clean(self,extracted_data):    \n",
    "        \"\"\"A function to celan the extracted data\n",
    "        The result is a dictionary of dataframes \"\"\" \n",
    "        #Transformer les données extraites et les mettres dans dataframes Spark   \n",
    "        flights_df = spark.createDataFrame(extracted_data[\"flights\"])\n",
    "        airlines_df = spark.createDataFrame(extracted_data[\"airlines\"])\n",
    "        #Ajouter un nouveau dataframe qui va correspondre entre les compagnies et les vols actifs\n",
    "        flights_iata = flights_df.filter(flights_df['on_ground'] == 0).select('airline_iata', 'id') \n",
    "        flights_icao = flights_df.filter(flights_df['on_ground'] == 0).select('airline_icao', 'id')\n",
    "        airlines_tmp = airlines_df.toDF(*['airline_iata', 'airline_icao', 'airline_name']) # rename airlines colums\n",
    "        airlines_tmp_iata = airlines_tmp.join(flights_iata, 'airline_iata', how='left') # flights by iata \n",
    "        airlines_tmp_icao = airlines_tmp.join(flights_icao, 'airline_icao', how='left') # flights by icao\n",
    "        airlines_tmp_icao = airlines_tmp_icao.select('airline_iata', 'airline_icao', 'airline_name', 'id')\n",
    "        airlines_active_flights_df = airlines_tmp_iata.union(airlines_tmp_icao).drop_duplicates(subset=['id'])\n",
    "        \n",
    "        return {\n",
    "            \"flights\": flights_df,\n",
    "            \"airlines\": airlines_df,\n",
    "            \"airlines_flights\": airlines_active_flights_df\n",
    "        }\n",
    "\n",
    "    def add_timestamp_columns(self, df):\n",
    "        current_ts = F.current_timestamp()\n",
    "        df = df.withColumn(\"tech_year\", F.year(current_ts)) \\\n",
    "               .withColumn(\"tech_month\", F.month(current_ts)) \\\n",
    "               .withColumn(\"tech_day\", F.dayofmonth(current_ts)) \\\n",
    "               .withColumn(\"tech_time\", F.date_format(current_ts, 'HH:mm:ss'))\n",
    "        return df\n",
    "\n",
    "    def create_dataframes(self, extracted_data):\n",
    "        df_airports = self.convert_airports_to_df(extracted_data[\"airports\"])\n",
    "        df_zones = self.convert_zones_to_df(extracted_data[\"zones\"])\n",
    "\n",
    "        return df_airports, df_zones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac413c82-094c-477e-9fef-ae6a6b959978",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Créer une instance de FlightRadar24API\n",
    "fr_api = FlightRadar24API()\n",
    "\n",
    "# Créer une instance de DataExtractor en passant fr_api comme argument\n",
    "data_extractor = DataExtractor(fr_api)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435a2eee-2a98-4944-ae86-62beb40e1b5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#la durée d'extraction en secondes (2 minutes)\n",
    "extraction_duration = 120\n",
    "# l'heure de début\n",
    "start_time = time.time()\n",
    "#la collecte de données pendant la durée spécifiée\n",
    "while time.time() - start_time < extraction_duration:\n",
    "    extracted_data = data_extractor.extract_data()\n",
    "    extracted_data_FLIGHT = data_extractor.extract_data_from_dic()\n",
    "    time.sleep(10)  # Attendre 10 secondes avant la prochaine collecte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d40dcd-f1ce-4f81-bb0d-69cb8ca826bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Appelez la méthode clean pour nettoyer les données et générer les nouveaux dataframes nettoyés\n",
    "cleaned_data = data_extractor.clean(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59afc4d6-f572-439b-a165-7f190fda2dde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_flights = cleaned_data[\"flights\"]\n",
    "df_airlines = cleaned_data[\"airlines\"]\n",
    "df_airlines_active_flights = cleaned_data[\"airlines_flights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76be52f-a129-43f0-bb83-9e6b08aea6e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logging.info(\"Transforming extracted data into DataFrames...\")\n",
    "# Appelez la méthode create_dataframes pour créer des DataFrames à partir des données extraites\n",
    "df_airports, df_zones = data_extractor.create_dataframes(extracted_data_FLIGHT)\n",
    "\n",
    "# Appliquez d'autres transformations sur les DataFrames si nécessaire\n",
    "df_flights = data_extractor.add_timestamp_columns(df_flights)\n",
    "df_airlines = data_extractor.add_timestamp_columns(df_airlines)\n",
    "df_airlines_active_flights = data_extractor.add_timestamp_columns(df_airlines_active_flights)\n",
    "df_airports = data_extractor.add_timestamp_columns(df_airports)\n",
    "df_zones = data_extractor.add_timestamp_columns(df_zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c012b246-7977-4ce4-af27-87311465a2e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Liste des noms des 5 continents que vous souhaitez conserver\n",
    "continents_a_garder = [\"oceania\", \"asia\", \"africa\", \"atlantic\", \"northatlantic\",\"europe\"]\n",
    "\n",
    "# Appliquez le filtrage pour ne conserver que les lignes correspondant aux continents spécifiés\n",
    "df_zone_filtré = df_zones.filter(df_zones[\"Zone Name\"].isin(continents_a_garder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce8654a-6261-4b0c-b3fc-f0f99787ae58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Ecriture des données en parquet, partitionnement par date ###\n",
    "\n",
    "logging.info(\"Writing DataFrames to parquet...\")\n",
    "def write_df_as_parquet(df, output_path):\n",
    "    df.write.mode(\"overwrite\").partitionBy(\"tech_year\", \"tech_month\", \"tech_day\").parquet(output_path)\n",
    " \n",
    "output_directory = \"/FileStore/Bronze/FlightsRadar_API\"  \n",
    "write_df_as_parquet(df_flights, output_directory  + '/flights')\n",
    "write_df_as_parquet(df_airlines, output_directory + '/airlines')\n",
    "write_df_as_parquet(df_airlines_active_flights, output_directory + '/airlines_active_flights')\n",
    "write_df_as_parquet(df_airports, output_directory + '/airports')\n",
    "write_df_as_parquet(df_zone_filtré, output_directory +'/zones')\n",
    " \n",
    "logging.info(\"All tasks completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extraction_Process_FlightRadar24_API",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
